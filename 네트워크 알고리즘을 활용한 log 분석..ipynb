{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분석 목적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "온라인 샵 유저의 이동 경로를 파악해, 구매 병목 현상이 일어나는 지점을 발견하고자 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분석 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>네트워크 알고리즘</b>을 활용해 유저의 이동 경로를 시각화 하여 파악.\n",
    "\n",
    "GA를 통해 유의미하다고 판단 된 척도로 소비자를 그룹핑해(6 group) 네트워크 분석 시행.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import pandas.io.sql as pdsql\n",
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "pd.set_option('max_columns', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"c:\\\\Users\\\\LG\\\\Desktop\\\\ipClassified.txt\", 'rb') as f:\n",
    "    ipClassified = pickle.load(f)  #구매,비구매 고객간 구별 위한 ip pickle file\n",
    "                                   #('210.105.33', 1) 형태로 0,1로 구매,비구매 구분\n",
    "\n",
    "#비구매 iplist 추출        \n",
    "NotBuy_IpList = [logip for (logip, _bool) in ipClassified if _bool == 0]\n",
    "\n",
    "query_logip = \\\n",
    "\"\"\"' or logip = '\"\"\".join(NotBuy_IpList)\n",
    "\n",
    "query = \\\n",
    "\"\"\"\n",
    "select before_url\n",
    "logip, logdate, before_url, after_url\n",
    "from LogIntake\n",
    "where logip = '{}'\n",
    "\"\"\".format(query_logip)\n",
    "\n",
    "print(\"logip: {}\".format(NotBuy_IpList[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB 접속\n",
    "conn = pyodbc.connect(driver='{SQL Server}', server='175.114.47.85', UID='LYJ_admin', PWD='dudwns06', database='intake')\n",
    "df = pdsql.read_sql_query(query, con=conn)\n",
    "conn.close()\n",
    "\n",
    "# 상위 1000개 ip의 로그만 추출\n",
    "df = df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log url 전처리(네트워크 알고리즘으로 시각화 위해 url 단축, 라벨링) \n",
    "#before_url과 after_url은 유저의 방문 기록을 나타내는 변수\n",
    "\n",
    "df['before_url'] = df['before_url'].apply(lambda x: re.sub('https://www.shopintake.com[\\/]*', '', x))\n",
    "df['after_url'] = df['after_url'].apply(lambda x: re.sub('https://www.shopintake.com[\\/]*', '', x))\n",
    "\n",
    "df['before_url'] = df['before_url'].apply(lambda x: re.sub('\\/app\\/auth\\/login\\/\\?token=(.*)url=', '', x))\n",
    "df['after_url'] = df['after_url'].apply(lambda x: re.sub('\\/app\\/auth\\/login\\/\\?token=(.*)url=', '', x))\n",
    "\n",
    "df['before_url'] = df['before_url'].apply(lambda x: re.sub('\\/app\\/notice\\/\\?os_type=ios', '', x))\n",
    "df['after_url'] = df['after_url'].apply(lambda x: re.sub('\\/app\\/notice\\/\\?os_type=ios', '', x))\n",
    "\n",
    "df['before_url'] = df['before_url'].apply(lambda x: re.sub('\\?(.*)', '', x))\n",
    "df['after_url'] = df['after_url'].apply(lambda x: re.sub('\\?(.*)', '', x))\n",
    "\n",
    "df['before_url'] = df['before_url'].apply(lambda x: re.sub('static(.*)', '', x))\n",
    "df['after_url'] = df['after_url'].apply(lambda x: re.sub('static(.*)', '', x))\n",
    "\n",
    "df = df[df['before_url'] != '']\n",
    "df = df[df['after_url'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 전처리\n",
    "df['before_url'] = df['before_url'].apply(lambda x: re.sub(\"view\\/[0-9]*/\", \"view/number/\", x))\n",
    "df['after_url'] = df['after_url'].apply(lambda x: re.sub(\"view\\/[0-9]*/\", \"view/number/\", x))\n",
    "\n",
    "df['before_url'] = df['before_url'].apply(lambda x: re.sub(\"category\\/[A-Za-z_]*/\", \"category/subcategory/\", x))\n",
    "df['after_url'] = df['after_url'].apply(lambda x: re.sub(\"category\\/[A-Za-z_]*/\", \"category/subcategory/\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beforeUrlList = df['before_url'].tolist()\n",
    "afterUrlList = df['after_url'].tolist()\n",
    "\n",
    "# 전처리 된 url 간 발생한 모든 접속 경로를 표현하기 위해 edge와 node 생성. \n",
    "# 접속 경로는 (before_url,after_url) 형태로 짝지어 표현한다.\n",
    "nodeList = list(set(beforeUrlList + afterUrlList))\n",
    "edgeList = list(zip(beforeUrlList, afterUrlList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#네트워크 분석\n",
    "\n",
    "nodes = []\n",
    "edges = []\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_nodes_from(nodeList)\n",
    "G.add_edges_from(edgeList)\n",
    "\n",
    "# edge link로 필터링\n",
    "edgeCount = Counter(edgeList)\n",
    "remove_by_edge = [(before, after) for (before, after) in edgeCount.keys() if edgeCount[(before, after)] < 10]\n",
    "G.remove_edges_from(remove_by_edge)\n",
    "\n",
    "# degree로 필터링\n",
    "remove_by_degree = [node for node, degree in nx.degree(G) if degree == 0]\n",
    "G.remove_nodes_from(remove_by_degree)\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "nx.draw_networkx(G, pos)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
